{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1278cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /root/miniconda3/envs/movinet/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting remotezip\n",
      "  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting opencv-python==4.5.2.52\n",
      "  Downloading opencv_python-4.5.2.52-cp38-cp38-manylinux2014_x86_64.whl (51.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python-headless==4.5.2.52\n",
      "  Downloading opencv_python_headless-4.5.2.52-cp38-cp38-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tf-models-official\n",
      "  Downloading tf_models_official-2.11.3-py2.py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from opencv-python==4.5.2.52) (1.24.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from remotezip) (2.28.2)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.6.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.33-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml<6.0,>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gin-config\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting immutabledict\n",
      "  Downloading immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
      "Requirement already satisfied: six in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tf-models-official) (1.16.0)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow~=2.11.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tf-models-official) (2.11.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.79.0-py2.py3-none-any.whl (11.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tf-models-official) (1.5.3)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tf-models-official) (5.9.0)\n",
      "Collecting tensorflow-text~=2.11.0\n",
      "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.16.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: urllib3 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from pandas>=0.22.0->tf-models-official) (2022.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (3.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (4.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.6.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (65.6.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: libclang>=13.0.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (15.0.6.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (0.30.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (1.51.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.2)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (2.11.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (23.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow~=2.11.0->tf-models-official) (23.1.21)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Using cached dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from matplotlib->tf-models-official) (5.2.0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from oauth2client->tf-models-official) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from oauth2client->tf-models-official) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from oauth2client->tf-models-official) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from requests->remotezip) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from requests->remotezip) (3.4)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting regex\n",
      "  Using cached regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: lxml in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from sacrebleu->tf-models-official) (4.9.1)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Using cached etils-1.0.0-py3-none-any.whl (146 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: toml in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official) (0.37.1)\n",
      "Requirement already satisfied: zipp in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (3.13.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (5.3.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.8.1)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (2.1.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/miniconda3/envs/movinet/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.2.2)\n",
      "Building wheels for collected packages: remotezip, kaggle, pycocotools, seqeval, promise\n",
      "  Building wheel for remotezip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7933 sha256=8831651c40b1eb07b5571198028276a7fb2ed4fd3c57d2c26f55e2c619a270e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/f9/e9/dd/c7cb0457b94c0cd8a45c905a7993df1465df4730b41172337c\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73031 sha256=6e65deb4e9d941cc1f7d3f8543e6e9cf519e0699ded8034a836d568b0c877d32\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/f3/c7/fc5a63bb33d22177609b06c5b4c714b5eb3f1b195ce9dc5e47\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl size=425664 sha256=6719f8e6892dfb9b5194bee2d5ec62b33634b59c85cc01c2b901bb332c1ad09c\n",
      "  Stored in directory: /root/.cache/pip/wheels/07/42/0a/26d14b3a7343223fcee09c101e73d82fff8bdd1541d85fe033\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=0a4fa9d36d527f708a7b4907cccd73631750d5060da1fc6785f6f9c2d9598cd2\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/30/9b/6b670dac34775f2b7cc4e9b172202e81fbb4f9cdb103c1ca66\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=703c3ca481e0bbba8fd84ba7e0dfb1eb215ed261525c3511f9349ca4917d689a\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/fe/dc/a7b3e03dfd0afb3a19691905bbafac1fbaebb704a02a4daeb2\n",
      "Successfully built remotezip kaggle pycocotools seqeval promise\n",
      "Installing collected packages: text-unidecode, sentencepiece, py-cpuinfo, gin-config, dm-tree, uritemplate, typeguard, tqdm, threadpoolctl, tf-slim, tensorflow-model-optimization, tensorflow-hub, tabulate, scipy, regex, pyyaml, python-slugify, pyparsing, promise, portalocker, Pillow, opencv-python-headless, opencv-python, kiwisolver, joblib, immutabledict, googleapis-common-protos, fonttools, etils, dill, Cython, cycler, contourpy, colorama, click, tensorflow-metadata, tensorflow-addons, scikit-learn, sacrebleu, remotezip, matplotlib, kaggle, httplib2, seqeval, pycocotools, oauth2client, google-auth-httplib2, google-api-core, tensorflow-datasets, google-api-python-client, tensorflow-text, tf-models-official\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.6.0.66\n",
      "    Uninstalling opencv-python-4.6.0.66:\n",
      "      Successfully uninstalled opencv-python-4.6.0.66\n",
      "Successfully installed Cython-0.29.33 Pillow-9.4.0 click-8.1.3 colorama-0.4.6 contourpy-1.0.7 cycler-0.11.0 dill-0.3.6 dm-tree-0.1.8 etils-1.0.0 fonttools-4.38.0 gin-config-0.5.0 google-api-core-2.11.0 google-api-python-client-2.79.0 google-auth-httplib2-0.1.0 googleapis-common-protos-1.58.0 httplib2-0.21.0 immutabledict-2.2.3 joblib-1.2.0 kaggle-1.5.12 kiwisolver-1.4.4 matplotlib-3.7.0 oauth2client-4.1.3 opencv-python-4.5.2.52 opencv-python-headless-4.5.2.52 portalocker-2.7.0 promise-2.3 py-cpuinfo-9.0.0 pycocotools-2.0.6 pyparsing-3.0.9 python-slugify-8.0.1 pyyaml-5.4.1 regex-2022.10.31 remotezip-0.12.1 sacrebleu-2.3.1 scikit-learn-1.2.1 scipy-1.10.1 sentencepiece-0.1.97 seqeval-1.2.2 tabulate-0.9.0 tensorflow-addons-0.19.0 tensorflow-datasets-4.8.2 tensorflow-hub-0.12.0 tensorflow-metadata-1.12.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 text-unidecode-1.3 tf-models-official-2.11.3 tf-slim-1.1.0 threadpoolctl-3.1.0 tqdm-4.64.1 typeguard-2.13.3 uritemplate-4.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bde4bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 01:49:56.038104: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 01:50:06.130546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/root/miniconda3/envs/movinet/lib/\n",
      "2023-02-28 01:50:06.130827: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/root/miniconda3/envs/movinet/lib/\n",
      "2023-02-28 01:50:06.130834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
    "import tensorflow_datasets as tfds\n",
    "from official.vision.configs import video_classification\n",
    "from official.projects.movinet.configs import movinet as movinet_configs\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_layers\n",
    "from official.projects.movinet.modeling import movinet_model\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Data Generator inherited from keras.utils.Sequence\n",
    "    Args: \n",
    "        directory: the path of data set, and each sub-folder will be assigned to one class\n",
    "        batch_size: the number of data points in each batch\n",
    "        shuffle: whether to shuffle the data per epoch\n",
    "    Note:\n",
    "        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n",
    "        # Initialize the params\n",
    "        self.batch_size = batch_size\n",
    "        self.directory = directory\n",
    "        self.shuffle = shuffle\n",
    "        self.data_aug = data_augmentation\n",
    "        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n",
    "        self.X_path, self.Y_dict = self.search_data() \n",
    "        # Print basic statistics information\n",
    "        self.print_stats()\n",
    "        return None\n",
    "        \n",
    "    def search_data(self):\n",
    "        X_path = []\n",
    "        Y_dict = {}\n",
    "        # list all kinds of sub-folders\n",
    "        self.dirs = sorted(os.listdir(self.directory))\n",
    "        one_hots = np_utils.to_categorical(range(len(self.dirs)))\n",
    "        for i,folder in enumerate(self.dirs):\n",
    "            folder_path = os.path.join(self.directory,folder)\n",
    "            for file in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path,file)\n",
    "                # append the each file path, and keep its label  \n",
    "                X_path.append(file_path)\n",
    "                Y_dict[file_path] = one_hots[i]\n",
    "        return X_path, Y_dict\n",
    "    \n",
    "    def print_stats(self):\n",
    "        # calculate basic information\n",
    "        self.n_files = len(self.X_path)\n",
    "        self.n_classes = len(self.dirs)\n",
    "        self.indexes = np.arange(len(self.X_path))\n",
    "        np.random.shuffle(self.indexes)\n",
    "        # Output states\n",
    "        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.n_classes))\n",
    "        for i,label in enumerate(self.dirs):\n",
    "            print('%10s : '%(label),i)\n",
    "        return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        # calculate the iterations of each epoch\n",
    "        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n",
    "        return int(steps_per_epoch)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get the data of each batch\n",
    "        \"\"\"\n",
    "        # get the indexs of each batch\n",
    "        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # using batch_indexs to get path of current batch\n",
    "        batch_path = [self.X_path[k] for k in batch_indexs]\n",
    "        # get batch data\n",
    "        batch_x, batch_y = self.data_generation(batch_path)\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # shuffle the data at each end of epoch\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, batch_path):\n",
    "        # load data into memory, you can change the np.load to any method you want\n",
    "        batch_x = [self.load_data(x) for x in batch_path]\n",
    "        batch_y = [self.Y_dict[x] for x in batch_path]\n",
    "        # transfer the data format and take one-hot coding for labels\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = np.array(batch_y)\n",
    "        return batch_x, batch_y\n",
    "      \n",
    "    def normalize(self, data):\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        return (data-mean) / std\n",
    "    \n",
    "    def random_flip(self, video, prob):\n",
    "        s = np.random.rand()\n",
    "        if s < prob:\n",
    "            video = np.flip(m=video, axis=2)\n",
    "        return video    \n",
    "    \n",
    "    def uniform_sampling(self, video, target_frames=64):\n",
    "        # get total frames of input video and calculate sampling interval \n",
    "        len_frames = int(len(video))\n",
    "        interval = int(np.ceil(len_frames/target_frames))\n",
    "        # init empty list for sampled video and \n",
    "        sampled_video = []\n",
    "        for i in range(0,len_frames,interval):\n",
    "            sampled_video.append(video[i])     \n",
    "        # calculate numer of padded frames and fix it \n",
    "        num_pad = target_frames - len(sampled_video)\n",
    "        padding = []\n",
    "        if num_pad>0:\n",
    "            for i in range(-num_pad,0):\n",
    "                try: \n",
    "                    padding.append(video[i])\n",
    "                except:\n",
    "                    padding.append(video[0])\n",
    "            sampled_video += padding     \n",
    "        # get sampled video\n",
    "        return np.array(sampled_video, dtype=np.float32)\n",
    "    \n",
    "    def random_clip(self, video, target_frames=64):\n",
    "        start_point = np.random.randint(len(video)-target_frames)\n",
    "        return video[start_point:start_point+target_frames]\n",
    "    \n",
    "    def dynamic_crop(self, video):\n",
    "        # extract layer of optical flow from video\n",
    "        opt_flows = video[...,3]\n",
    "        # sum of optical flow magnitude of individual frame\n",
    "        magnitude = np.sum(opt_flows, axis=0)\n",
    "        # filter slight noise by threshold \n",
    "        thresh = np.mean(magnitude)\n",
    "        magnitude[magnitude<thresh] = 0\n",
    "        # calculate center of gravity of magnitude map and adding 0.001 to avoid empty value\n",
    "        x_pdf = np.sum(magnitude, axis=1) + 0.001\n",
    "        y_pdf = np.sum(magnitude, axis=0) + 0.001\n",
    "        # normalize PDF of x and y so that the sum of probs = 1\n",
    "        x_pdf /= np.sum(x_pdf)\n",
    "        y_pdf /= np.sum(y_pdf)\n",
    "        # randomly choose some candidates for x and y \n",
    "        x_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=x_pdf)\n",
    "        y_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=y_pdf)\n",
    "        # get the mean of x and y coordinates for better robustness\n",
    "        x = int(np.mean(x_points))\n",
    "        y = int(np.mean(y_points))\n",
    "        # avoid to beyond boundaries of array\n",
    "        x = max(56,min(x,167))\n",
    "        y = max(56,min(y,167))\n",
    "        # get cropped video \n",
    "        return video[:,x-56:x+56,y-56:y+56,:]  \n",
    "    \n",
    "    def color_jitter(self,video):\n",
    "        # range of s-component: 0-1\n",
    "        # range of v component: 0-255\n",
    "        s_jitter = np.random.uniform(-0.2,0.2)\n",
    "        v_jitter = np.random.uniform(-30,30)\n",
    "        for i in range(len(video)):\n",
    "            hsv = cv2.cvtColor(video[i], cv2.COLOR_RGB2HSV)\n",
    "            s = hsv[...,1] + s_jitter\n",
    "            v = hsv[...,2] + v_jitter\n",
    "            s[s<0] = 0\n",
    "            s[s>1] = 1\n",
    "            v[v<0] = 0\n",
    "            v[v>255] = 255\n",
    "            hsv[...,1] = s\n",
    "            hsv[...,2] = v\n",
    "            video[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "        return video\n",
    "        \n",
    "    def load_data(self, path):\n",
    "        data = np.load(path, mmap_mode='r')[...,:3]\n",
    "        data = np.float32(data)\n",
    "        #data=tf.image.resize(data, (172, 172)) #uncomment if you are using a0 or a1\n",
    "        # sampling 64 frames uniformly from the entire video\n",
    "        data = self.uniform_sampling(video=data, target_frames=64)\n",
    "        # whether to utilize the data augmentation\n",
    "        if  self.data_aug:\n",
    "            data = self.color_jitter(data)\n",
    "            data = self.random_flip(data, prob=0.5)\n",
    "        # normalize\n",
    "        data = tf.cast(data,tf.float32) / 255\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'a2' #---> You can change this for a0 (light), or a2 (robust)\n",
    "resolution = 224 #for a0 and a1 the resolution is 172\n",
    "\n",
    " #Load pre-trained weights, be sure you change de model id\n",
    "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
    "!tar -xvf movinet_a0_base.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0305f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f5718e67ee0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "backbone = movinet.Movinet(\n",
    "    model_id=model_id,\n",
    "#     causal=True,\n",
    "#     conv_type='3d',\n",
    "#     se_type='3d',\n",
    "    activation='hard_swish',\n",
    "    gating_activation='hard_sigmoid'\n",
    ")\n",
    "backbone.trainable = False\n",
    "\n",
    "# Set num_classes=600 to load the pre-trained weights from the original model\n",
    "model = movinet_model.MovinetClassifier(\n",
    "    backbone, num_classes=600)\n",
    "model.build([1, 1, 1, 1, 3])\n",
    "\n",
    "# Load pre-trained weights, be sure you change de model id\n",
    "\n",
    "checkpoint_dir = f'movinet_{model_id}_base'\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(checkpoint_path)\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a69f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "num_frames=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7aad6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes, freeze_backbone=False):\n",
    "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "  model = movinet_model.MovinetClassifier(\n",
    "      backbone=backbone,\n",
    "      num_classes=num_classes)\n",
    "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e01cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier(batch_size, num_frames, resolution, backbone, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15f0e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"movinet_classifier_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image (InputLayer)          [(None, None, None, None  0         \n",
      "                             , 3)]                               \n",
      "                                                                 \n",
      " movinet (Movinet)           ({'stem': (None, None, N  2738754   \n",
      "                             one, None, 16),                     \n",
      "                              'block0_layer0': (None,            \n",
      "                              None, None, None, 16),             \n",
      "                              'block0_layer1': (None,            \n",
      "                              None, None, None, 16),             \n",
      "                              'block0_layer2': (None,            \n",
      "                              None, None, None, 16),             \n",
      "                              'block1_layer0': (None,            \n",
      "                              None, None, None, 40),             \n",
      "                              'block1_layer1': (None,            \n",
      "                              None, None, None, 40),             \n",
      "                              'block1_layer2': (None,            \n",
      "                              None, None, None, 40),             \n",
      "                              'block1_layer3': (None,            \n",
      "                              None, None, None, 40),             \n",
      "                              'block1_layer4': (None,            \n",
      "                              None, None, None, 40),             \n",
      "                              'block2_layer0': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block2_layer1': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block2_layer2': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block2_layer3': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block2_layer4': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block3_layer0': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block3_layer1': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block3_layer2': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block3_layer3': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block3_layer4': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block3_layer5': (None,            \n",
      "                              None, None, None, 72),             \n",
      "                              'block4_layer0': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'block4_layer1': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'block4_layer2': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'block4_layer3': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'block4_layer4': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'block4_layer5': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'block4_layer6': (None,            \n",
      "                              None, None, None, 144),            \n",
      "                              'head': (None, None, No            \n",
      "                             ne, None, 640)},                    \n",
      "                              {'state_block0_layer0_p            \n",
      "                             ool_buffer': (None, None            \n",
      "                             , None, None, 40),                  \n",
      "                              'state_block0_layer0_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block0_layer1_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 40),                   \n",
      "                              'state_block0_layer1_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block0_layer2_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 64),                   \n",
      "                              'state_block0_layer2_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block1_layer0_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 96),                   \n",
      "                              'state_block1_layer0_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block1_layer1_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 120),                  \n",
      "                              'state_block1_layer1_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block1_layer2_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 96),                   \n",
      "                              'state_block1_layer2_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block1_layer3_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 96),                   \n",
      "                              'state_block1_layer3_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block1_layer4_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 120),                  \n",
      "                              'state_block1_layer4_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block2_layer0_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block2_layer0_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block2_layer1_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 160),                  \n",
      "                              'state_block2_layer1_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block2_layer2_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block2_layer2_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block2_layer3_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 192),                  \n",
      "                              'state_block2_layer3_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block2_layer4_po            \n",
      "                             ol_buffer': (None, None,            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              None, None, 240),                  \n",
      "                              'state_block2_layer4_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block3_layer0_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block3_layer0_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block3_layer1_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block3_layer1_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block3_layer2_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block3_layer2_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block3_layer3_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block3_layer3_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block3_layer4_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 144),                  \n",
      "                              'state_block3_layer4_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block3_layer5_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 240),                  \n",
      "                              'state_block3_layer5_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer0_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 480),                  \n",
      "                              'state_block4_layer0_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer1_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 384),                  \n",
      "                              'state_block4_layer1_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer2_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 384),                  \n",
      "                              'state_block4_layer2_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer3_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 480),                  \n",
      "                              'state_block4_layer3_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer4_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 480),                  \n",
      "                              'state_block4_layer4_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer5_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 480),                  \n",
      "                              'state_block4_layer5_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_block4_layer6_po            \n",
      "                             ol_buffer': (None, None,            \n",
      "                              None, None, 576),                  \n",
      "                              'state_block4_layer6_po            \n",
      "                             ol_frame_count': (1,),              \n",
      "                              'state_head_pool_buffer            \n",
      "                             ': (None, None, None, No            \n",
      "                             ne, 640),                           \n",
      "                              'state_head_pool_frame_            \n",
      "                             count': (1,)})                      \n",
      "                                                                 \n",
      " classifier_head_1 (Classifi  (None, 2)                1316866   \n",
      " erHead)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,055,620\n",
      "Trainable params: 1,316,866\n",
      "Non-trainable params: 2,738,754\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b2f359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 files belonging to 2 classes.\n",
      "     Fight :  0\n",
      "  NonFight :  1\n",
      "Found 400 files belonging to 2 classes.\n",
      "     Fight :  0\n",
      "  NonFight :  1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"- init data generator\"\"\"\n",
    "\n",
    "dataset = 'Dataset'\n",
    "\n",
    "train_generator = DataGenerator(directory='./{}/train'.format(dataset), \n",
    "                                batch_size=batch_size, \n",
    "                                data_augmentation=True)\n",
    "\n",
    "val_generator = DataGenerator(directory='./{}/val'.format(dataset),\n",
    "                              batch_size=batch_size, \n",
    "                              data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bb3f95ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for frames, labels in train_generator:\n",
    "  print(labels)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d605a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_steps = len(train_generator)\n",
    "total_train_steps = train_steps * num_epochs\n",
    "test_steps = len(val_generator)\n",
    "\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=loss_obj, optimizer=optimizer, metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6346c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "import keras\n",
    "\n",
    "class MyCbk(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model):\n",
    "         self.model_to_save = model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model_to_save.save('Logs_MovinetA2_base/model_at_epoch_%d.h5' % (epoch+1))\n",
    "\n",
    "check_point = MyCbk(model)\n",
    "\n",
    "\n",
    "filename = 'Logs_MovinetA2_base/ours_log.csv'\n",
    "csv_logger = CSVLogger(filename, separator=',', append=True)\n",
    "\n",
    "callbacks_list = [check_point, csv_logger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72c2dc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.3776 - accuracy: 0.8256 - val_loss: 0.3173 - val_accuracy: 0.8625\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 400s 2s/step - loss: 0.2830 - accuracy: 0.8813 - val_loss: 0.3260 - val_accuracy: 0.8650\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 394s 2s/step - loss: 0.2525 - accuracy: 0.8975 - val_loss: 0.3169 - val_accuracy: 0.8600\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 394s 2s/step - loss: 0.2279 - accuracy: 0.9106 - val_loss: 0.3520 - val_accuracy: 0.8550\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 396s 2s/step - loss: 0.2408 - accuracy: 0.8981 - val_loss: 0.3386 - val_accuracy: 0.8575\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 398s 2s/step - loss: 0.2014 - accuracy: 0.9175 - val_loss: 0.3464 - val_accuracy: 0.8625\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 392s 2s/step - loss: 0.2140 - accuracy: 0.9125 - val_loss: 0.3822 - val_accuracy: 0.8450\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 399s 2s/step - loss: 0.1892 - accuracy: 0.9200 - val_loss: 0.3792 - val_accuracy: 0.8625\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.1849 - accuracy: 0.9206 - val_loss: 0.3671 - val_accuracy: 0.8625\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 360s 2s/step - loss: 0.1739 - accuracy: 0.9287 - val_loss: 0.3691 - val_accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=num_epochs,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_steps=test_steps,\n",
    "                    validation_freq=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f673fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
